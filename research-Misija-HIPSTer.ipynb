{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95VGNi8LbVSW"
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KffoFNIPazQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install sentence-transformers hf_xet\n",
    "#jina-embeddings benefit from FlashAttention-2\n",
    "#!pip install flash-attn --no-build-isolation\n",
    "#!pip install -v -U flash-attn\n",
    "!pip install ninja pyarrow\n",
    "!pip install pyod catboost\n",
    "!pip install ftfy emoji einops\n",
    "!pip install jupyter_capture_output\n",
    "!pip install --no-deps dask-expr\n",
    "!pip uninstall -y scipy\n",
    "!pip cache purge  # Clear cached versions\n",
    "!pip install --upgrade --force-reinstall scipy==1.11.4\n",
    "!pip install betacal roc_utils\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95EHx8dbbVSX"
   },
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEETDAGBbVSY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time, os, re, torch, ftfy, emoji, jupyter_capture_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from tqdm.dask import TqdmCallback\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from pyod.models.hbos import HBOS\n",
    "from catboost import CatBoostClassifier\n",
    "from betacal import BetaCalibration\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score, cohen_kappa_score, precision_recall_curve, average_precision_score\n",
    "from roc_utils import *\n",
    "import seaborn as sns\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"flash_attn.ops.triton.layer_norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EqR3mfKbVSY"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAdTUDuEd4KI"
   },
   "outputs": [],
   "source": [
    "# research code for HIPSTer project: Hybrid, Information, Psychological, Societal Threats\n",
    "# handling system for public security domain practitioners, businesses, and education (HIPSTer)\n",
    "\n",
    "dataset = \"LtHate\" # choose text dataset: LtHate RuToxic DynaHate\n",
    "datasetFolder = './data/'\n",
    "resultsFolder = './results/'\n",
    "vectors = [\"gte\", \"snow\", \"jina\", \"e5\"] # choose among modern vectorizers: gte snow jina e5\n",
    "chunk_size = 1024 # large (if gte not used or many texts) or small (if GPU resources are limited)\n",
    "batch_size_setting = 64 # large (if gte not used or many texts) or small (if GPU resources are limited)\n",
    "compDevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "useDimensionalityReduction = True\n",
    "useCatBoostOutputCallibration = True\n",
    "\n",
    "k = 10 # Number of folds for cross-validation with StratifiedKFold, i.e. 10-fold CV\n",
    "num_vars = 64 # Number of variables after dimensionality reduction with PCA\n",
    "num_tree = 200 # Maximum possible number of trees to grow for CatBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvLogFilename = resultsFolder + dataset + \"-CV-log.txt\"\n",
    "resultsTableFile = resultsFolder + dataset + \"-table.txt\"\n",
    "resultsTableSummary = resultsFolder + dataset + \"-table.csv\"\n",
    "rocPlotFilename = resultsFolder + dataset + \"-fig-ROC.png\"\n",
    "prcPlotFilename = resultsFolder + dataset + \"-fig-PRC.png\"\n",
    "if not os.path.exists(resultsFolder):\n",
    "    os.makedirs(resultsFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyDT5_SabVSZ"
   },
   "outputs": [],
   "source": [
    "def fix_punctuation(text, toneDown=True):\n",
    "    # First, use ftfy to fix any encoding issues\n",
    "    if hasattr(text, '__len__'):\n",
    "        text = ftfy.fix_text(text)\n",
    "\n",
    "        # Custom rules for punctuation fixing\n",
    "        rules = [\n",
    "            # Remove http and https links\n",
    "            (r'https?://\\S+', ''),\n",
    "            # Remove consecutive repetitive punctuation, but keep a maximum of two for emphasis (e.g., !!)\n",
    "            (r'([,\\.?!])\\1{2,}', r'\\1\\1'),\n",
    "            # Add space after comma, period, question mark, or exclamation mark if not followed by space\n",
    "            (r'([,\\.?!])(?=[^\\s])', r'\\1 '),\n",
    "            # Remove space before comma, period, question mark, or exclamation mark\n",
    "            (r'\\s+([,\\.?!])', r'\\1'),\n",
    "            # Fix multiple spaces\n",
    "            (r'\\s{2,}', ' '),\n",
    "            # Ensure numbers have space before and after, except when punctuation or hyphen follows\n",
    "            (r'(\\d)(?=[^\\s\\d,\\.?!-])', r'\\1 '),\n",
    "            (r'(?<=[^\\s\\d-])(\\d)', r' \\1')\n",
    "        ]\n",
    "\n",
    "        if toneDown:\n",
    "            rules.append((r'[?!]', '.'))\n",
    "\n",
    "        # Replace emoji with :shortcode:\n",
    "        text = emoji.demojize(text, delimiters=(\" ::\", \":: \"))\n",
    "\n",
    "        # Apply each rule\n",
    "        for pattern, replacement in rules:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "\n",
    "        text = text.strip()\n",
    "    else:\n",
    "        text = ''\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_vectorize(df, fix_punctuation, sentvec=\"e5\", device=\"cuda\", normalize_embeddings=False, chunk_size=256, batch_size=32):\n",
    "    \"\"\"\n",
    "    Cleans text data and vectorizes it using a sentence transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input pandas DataFrame containing text data.\n",
    "    - fix_punctuation (function): Function to preprocess and fix punctuation in text.\n",
    "    - sentvec (str): Identifier for sentence vectorization method, e.g., \"e5-large-instruct\".\n",
    "    - device (str, default=\"cuda\"): Device to use for the sentence transformer model, e.g., \"cpu\" or \"cuda\".\n",
    "    - normalize_embeddings (bool): Whether to normalize embeddings.\n",
    "    - chunk_size (int, default=1024): Number of text samples per chunk for encoding.\n",
    "    - batch_size (int, default=128): Batch size for encoding in the transformer.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the resulting embeddings with appropriate column names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the sentence transformer model based on `sentvec`    \n",
    "    if sentvec == \"jina\":\n",
    "        st = SentenceTransformer('jinaai/' + sentvec + '-embeddings-v3', trust_remote_code=True, device=device)\n",
    "    elif sentvec == \"snow\":\n",
    "        st = SentenceTransformer('Snowflake/' + sentvec + 'flake-arctic-embed-l-v2.0', device=device)    \n",
    "    elif sentvec == \"gte\":\n",
    "        st = SentenceTransformer('Alibaba-NLP/' + sentvec + '-Qwen2-1.5B-instruct', device=device)    \n",
    "    else: # \"e5\"\n",
    "        st = SentenceTransformer('intfloat/multilingual-' + sentvec + '-large-instruct', device=device)\n",
    "\n",
    "    # Partition data for parallel processing\n",
    "    n_partitions = mp.cpu_count()\n",
    "    ddf = dd.from_pandas(df, npartitions=n_partitions)\n",
    "\n",
    "    with TqdmCallback(desc=\"Cleaning text\"):\n",
    "        texts = ddf.apply(lambda x: fix_punctuation(x.iloc[1]), axis=1, meta=pd.Series(dtype=\"str\")).compute(scheduler=\"processes\").tolist()\n",
    "\n",
    "    # Determine prompt for the selected sentence vectorization method\n",
    "    st_prompt = \"query: \" if sentvec == \"e5\" else \"\"\n",
    "\n",
    "    # Process texts in chunks for memory efficiency\n",
    "    results = []\n",
    "    total_chunks = (len(texts) + chunk_size - 1) // chunk_size  # Calculate total number of chunks\n",
    "    width = len(str(total_chunks))  # Width for chunk number formatting\n",
    "\n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache if necessary\n",
    "        current_chunk = i // chunk_size + 1  # Current chunk number\n",
    "        print(f\"\\rChunk: {current_chunk:0{width}}/{total_chunks:0{width}} \", end='', flush=True)\n",
    "        chunk = texts[i:i + chunk_size]\n",
    "        result = st.encode(chunk, batch_size=batch_size, normalize_embeddings=normalize_embeddings, show_progress_bar=True)\n",
    "        results.append(result)\n",
    "\n",
    "    # Concatenate results and format as DataFrame\n",
    "    X = np.concatenate(results, axis=0)\n",
    "    df_embeddings = pd.DataFrame(X)\n",
    "    df_embeddings.columns = [f'X{i+1}' for i in range(df_embeddings.shape[1])]\n",
    "\n",
    "    return df_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load text comments dataset\n",
    "if dataset == 'DynaHate':\n",
    "    df = pd.read_csv(datasetFolder + 'DynaHate.csv', engine='python', usecols=['text','label'])\n",
    "    df = df[df.columns[::-1]]\n",
    "    df.columns = [0, 1]\n",
    "    di = {\"nothate\": 0, \"hate\": 1}\n",
    "    y = df[0].map(di)\n",
    "elif dataset == 'LtHate':\n",
    "    dfA = pd.read_csv(datasetFolder + 'LtHate.csv', engine='python')\n",
    "    dfA = dfA[dfA.columns[::-1]]\n",
    "    dfA.columns = [0, 1]\n",
    "    di = {\"No\": 0, \"Yes\": 1}\n",
    "    yA = dfA[0].map(di)\n",
    "    dfB = pd.read_csv(datasetFolder + 'Semantika_2.txt', sep='__', header=None, usecols=[2], engine='python')\n",
    "    dfB = dfB[2].str.split(\" \", n=1, expand=True)\n",
    "    di = {\"neutral\": 0, \"offensive\": 1}\n",
    "    yB = dfB[0].map(di)\n",
    "    df = pd.concat([dfA, dfB], ignore_index=True)\n",
    "    y = pd.concat([yA, yB], ignore_index=True)        \n",
    "elif dataset == 'LtEmocionalumas':\n",
    "    df = pd.read_csv(datasetFolder + 'LtEmocionalumas.csv', engine='python')\n",
    "    df = df[df.columns[::-1]]\n",
    "    df.columns = [0, 1]\n",
    "    di = {\"No\": 0, \"Low\": 0, \"Medium\": 1, \"High\": 1, \"Critical\": 1}\n",
    "    y = df[0].map(di)\n",
    "elif dataset == 'LtManipuliacijos':\n",
    "    excel_file = pd.ExcelFile(datasetFolder + 'LtManipuliacijos.xlsx')\n",
    "    all_comments = []\n",
    "    all_labels = []\n",
    "    for sheet_name in excel_file.sheet_names:\n",
    "        df = pd.read_excel(datasetFolder + 'LtManipuliacijos.xlsx', sheet_name=sheet_name, usecols=[1])\n",
    "        sheet_comments = df['Komentaras'].astype(str).str.strip()\n",
    "        sheet_comments = sheet_comments[sheet_comments != ''].tolist()\n",
    "        all_comments.extend(sheet_comments)\n",
    "        if sheet_name == 'Manipuliaciniai':\n",
    "            all_labels.extend([1] * len(sheet_comments))\n",
    "        else:\n",
    "            all_labels.extend([0] * len(sheet_comments))\n",
    "    df = pd.DataFrame({0: all_labels, 1: all_comments})\n",
    "    y = df[0]      \n",
    "elif dataset == 'RuToxic':\n",
    "    df = pd.read_csv(datasetFolder + 'RuToxic.csv', engine='python')\n",
    "    df = df[df.columns[::-1]]\n",
    "    df.columns = [0, 1]\n",
    "    y = df[0]\n",
    "else:\n",
    "    df = pd.read_csv('Semantika_2.txt', sep='__', header=None, usecols=[2], engine='python')\n",
    "    df = df[2].str.split(\" \", n=1, expand=True)\n",
    "    di = {\"neutral\": 0, \"offensive\": 1}\n",
    "    y = df[0].map(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPtFi-3ekCM-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Vectorize text data\n",
    "for sentvec in vectors:\n",
    "    xVarFilename = datasetFolder + dataset + \"-X-\" + sentvec + \".parquet\"\n",
    "    files_exist_condition = os.path.exists(xVarFilename)\n",
    "    if not files_exist_condition:\n",
    "        try:\n",
    "            print(\"Vectorizing text on GPU 0...\")\n",
    "            torch.cuda.set_device(0)\n",
    "            torch.cuda.empty_cache()\n",
    "            df_post = clean_and_vectorize(df, fix_punctuation, sentvec, compDevice, False, chunk_size, batch_size_setting)\n",
    "            df_post.to_parquet(xVarFilename, engine='pyarrow')\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(\"Vectorizing text on GPU 1...\")\n",
    "                torch.cuda.set_device(1)\n",
    "                torch.cuda.empty_cache()\n",
    "                df_post = clean_and_vectorize(df, fix_punctuation, sentvec, compDevice, False, chunk_size, batch_size_setting)\n",
    "                df_post.to_parquet(xVarFilename, engine='pyarrow')\n",
    "            else:\n",
    "                # Re-raise if it's not an OOM error\n",
    "                raise\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Read vectorized data\n",
    "XX = []\n",
    "for sentvec in vectors:\n",
    "    xVarFilename = datasetFolder + dataset + \"-X-\" + sentvec + \".parquet\"\n",
    "    df_post = dd.read_parquet(xVarFilename, engine='pyarrow')\n",
    "    XX.append(df_post.compute().to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOPdRD4sbVSZ"
   },
   "source": [
    "## Machine learning (CV using FOR loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoflip_score(y_true, y_scores):\n",
    "    # Calculate ROC AUC scores for both variants\n",
    "    auc_score = roc_auc_score(y_true, y_scores)\n",
    "    auc_score_inverted = roc_auc_score(y_true, 1 - y_scores)\n",
    "    # Determine which AUC is higher and return the corresponding scores\n",
    "    if auc_score > auc_score_inverted:\n",
    "        return y_scores\n",
    "    else:\n",
    "        return 1 - y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEdch6IFwT1J"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture_text --path $cvLogFilename\n",
    "\n",
    "X = range(len(y))\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "cb_task_type = 'GPU' if compDevice == 'cuda' else 'CPU' #torch.cuda.set_device(1)\n",
    "\n",
    "# Predicted scores to concatenate results across validation folds\n",
    "tst_idx, ground_truth = [], []\n",
    "os_scores = [[] for _ in vectors]  # One list per vectorizer\n",
    "cb_scores = [[] for _ in vectors]  # One list per vectorizer\n",
    "\n",
    "i = 0 # Perform the k-fold cross-validation\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    tst_idx.extend(test_index)\n",
    "    ground_truth.extend(y_test.tolist())\n",
    "\n",
    "    print(\"\\nCV fold %d/%d\" % (i + 1, k), flush=True)\n",
    "\n",
    "    os_fold_scores = []\n",
    "    cb_fold_scores = []\n",
    "\n",
    "    # One-class (1c) classification: Histogram-based outlier score (HBOS)\n",
    "    for j, sentvec in enumerate(vectors):\n",
    "        start = pd.Timestamp.now()\n",
    "        X_train, X_test = XX[j][train_index], XX[j][test_index]\n",
    "        if useDimensionalityReduction:\n",
    "            pca = PCA(n_components=num_vars)\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "        clf = HBOS(contamination=0.01)\n",
    "        clf.fit(X_train[y_train==1,:])\n",
    "        score = -1 * clf.decision_function(X_test) / 10000 # make score more aesthetic (calibration)\n",
    "        score = autoflip_score(y_test, score) # HBOS is 1c but ROC/PRC are for 2c so some sanity check\n",
    "        print(\"pyodHBOS %s AUC=%5.3f\" % (vectors[j], roc_auc_score(y_test, score)))\n",
    "        os_scores[j].extend(score.tolist())\n",
    "        print(str(pd.Timestamp.now()-start))\n",
    "        \n",
    "\n",
    "    # Two-class (2c) classification: CatBoost classifier (detection task)\n",
    "    for j, sentvec in enumerate(vectors):\n",
    "        start = pd.Timestamp.now()\n",
    "        X_train, X_test = XX[j][train_index], XX[j][test_index]\n",
    "        if useDimensionalityReduction:\n",
    "            pca = PCA(n_components=num_vars)\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "        model = CatBoostClassifier(iterations=num_tree, learning_rate=0.1, task_type=cb_task_type, allow_writing_files=False)\n",
    "        X_trn, X_val, y_trn, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=42)\n",
    "        model.fit(X_trn, y_trn, eval_set=(X_val, y_val), use_best_model=True, early_stopping_rounds=10, verbose=False)\n",
    "        score = model.predict_proba(X_test)[:, 1]\n",
    "        if useCatBoostOutputCallibration:\n",
    "            prob_pos_val = model.predict_proba(X_val)[:, 1]\n",
    "            beta_calibrator = BetaCalibration(parameters=\"abm\")\n",
    "            beta_calibrator.fit(prob_pos_val, y_val)\n",
    "            score = beta_calibrator.predict(score)\n",
    "    \n",
    "        print(\"catBoost %s AUC=%5.3f\" % (vectors[j], roc_auc_score(y_test, score)))\n",
    "        cb_scores[j].extend(score.tolist())\n",
    "        print(str(pd.Timestamp.now()-start))\n",
    "        \n",
    "    i = i + 1\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBi7z1PAbVSa"
   },
   "source": [
    "## Detection task results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC\n",
    "thrType = \"minopt\"\n",
    "showOpt = False\n",
    "roc_results = []\n",
    "fig, ax3 = plt.subplots()\n",
    "colors = sns.color_palette(\"pastel\", len(os_scores))\n",
    "for i, vec_scores in enumerate(os_scores):\n",
    "    roc = compute_roc(X=vec_scores, y=ground_truth, pos_label=1, objective=thrType)\n",
    "    roc_results.append(roc)\n",
    "    label = f\"1c {vectors[i]}\"\n",
    "    plot_roc(roc, label=label, color=colors[i], ax=ax3, show_opt=showOpt)\n",
    "for i, vec_scores in enumerate(cb_scores):\n",
    "    roc = compute_roc(X=vec_scores, y=ground_truth, pos_label=1, objective=thrType)\n",
    "    roc_results.append(roc)\n",
    "    label = f\"2c {vectors[i]}\"\n",
    "    plot_roc(roc, label=label, color=f\"C{i}\", ax=ax3, show_opt=showOpt)\n",
    "ax3.set_title(f\"{dataset} (n={len(y)}, target={100*np.sum(y==1)/len(y):.2f}%) → ROC\")\n",
    "ax3.legend()\n",
    "plt.show()\n",
    "fig.savefig(rocPlotFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PRC\n",
    "prc_results = []\n",
    "fig, ax3 = plt.subplots()\n",
    "colors = sns.color_palette(\"pastel\", len(os_scores))\n",
    "for i, vec_scores in enumerate(os_scores):\n",
    "    precision, recall, _ = precision_recall_curve(ground_truth, vec_scores, pos_label=1)\n",
    "    ap = average_precision_score(ground_truth, vec_scores, pos_label=1)\n",
    "    prc_results.append(ap)\n",
    "    label = f\"1c {vectors[i]} (AUC={ap:.3f})\"\n",
    "    ax3.plot(recall, precision, label=label, color=colors[i])\n",
    "for i, vec_scores in enumerate(cb_scores):\n",
    "    precision, recall, _ = precision_recall_curve(ground_truth, vec_scores, pos_label=1)\n",
    "    ap = average_precision_score(ground_truth, vec_scores, pos_label=1)\n",
    "    prc_results.append(ap)\n",
    "    label = f\"1c {vectors[i]} (AUC={ap:.3f})\"\n",
    "    ax3.plot(recall, precision, label=label, color=f\"C{i}\")\n",
    "ax3.set_title(f\"{dataset} (n={len(y)}, target={100*np.sum(y==1)/len(y):.2f}%) → PRC\")\n",
    "ax3.set_xlabel(\"Recall\")\n",
    "ax3.set_ylabel(\"Precision\")\n",
    "ax3.set_xlim([-0.02, 1.02])\n",
    "ax3.set_ylim([-0.02, 1.02])\n",
    "ax3.set_aspect('equal', adjustable='box')\n",
    "ax3.grid(color='darkgrey', linestyle='-', linewidth=0.5)\n",
    "ax3.legend(loc='lower left')\n",
    "plt.show()\n",
    "fig.savefig(prcPlotFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture_text --path $resultsTableFile\n",
    "\n",
    "# Constants\n",
    "kappa_line = \"cohens kappa                           %4.2f\\n\"\n",
    "title_line = '\\n-----------------------------------------------------\\n'\n",
    "\n",
    "# Ultra-compact score collections\n",
    "models = [('1c', os_scores), ('2c', cb_scores)]\n",
    "\n",
    "# Initialize list to store CSV data\n",
    "csv_data = []\n",
    "\n",
    "# Process all scores\n",
    "result_idx = 0\n",
    "for prefix, scores_list in models:\n",
    "    for i, vec_scores in enumerate(scores_list):\n",
    "        roc, ap = roc_results[result_idx], prc_results[result_idx]\n",
    "        variant_name = vectors[i]\n",
    "        \n",
    "        # Calculate predictions and metrics\n",
    "        predictions = vec_scores > roc.opd[thrType].opt\n",
    "        accuracy = accuracy_score(ground_truth, predictions)\n",
    "        kappa = cohen_kappa_score(ground_truth, predictions)\n",
    "        \n",
    "        # Print stylized results (original format)\n",
    "        print(f'\\n'\n",
    "              f'{prefix} {variant_name} : {dataset} AUC-ROC = {roc.auc:.3f}\\n'\n",
    "              f'{prefix} {variant_name} : {dataset} AUC-PRC = {ap:.3f}'\n",
    "              f'{title_line}'\n",
    "              f'{classification_report(ground_truth, predictions)}'\n",
    "              f'{kappa_line % kappa}'\n",
    "              f'{title_line}')\n",
    "        \n",
    "        # Collect data for CSV\n",
    "        csv_data.append({\n",
    "            'Model': f'{prefix} {variant_name}',\n",
    "            'Accuracy': f'{accuracy * 100:.2f} %',\n",
    "            'Kappa': kappa,\n",
    "            'AUC-ROC': roc.auc,\n",
    "            'AUC-PRC': ap\n",
    "        })\n",
    "              \n",
    "        result_idx += 1\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(csv_data)\n",
    "csv_filename = resultsTableSummary  # Using the resultsTableSummary string as filename\n",
    "df.to_csv(csv_filename, index=False, float_format='%.3f')\n",
    "print(f'\\nResults saved to CSV: {csv_filename}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
